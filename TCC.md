Anotações
https://dontpad.com/Kelton

Autores que tratam de computação em nuvem : "conversar c eles," concordando ou discordando.

Objetivo: no infinitivo e indicando o resultado. Com contexto.
Público: Para quem?
Contexto
Problema e Hipótese: uma pergunta feita pra confirmar, constatar a proposta de solução. Responde o problema.
Método:
Dedutivo - do geral pro particular - essa aqui q vamos usar ou
Hipotético dedutivo - essa aqui q vamos usar

1. Descobrimento do problema:
2. Colocação precisa do problema:
3. Conhecimentos/instrumentos relevantes ao problema:
4. Solução c/ auxílio dos meios identificados:
5. Novas ideias:
6. Obtenção da solução:
7. Investigação da solução obtida:
8. Prova:
9. Correção das hipóteses, teorias, procedimentos ou dados empregados na obtenção da solução incorreta:

Métodos:

Indutivo e suas etapas - do particular pro geral
Dedutivo - do geral pro particular - essa aqui q vamos usar
Hipotético dedutivo - essa aqui q vamos usar
Dialético
________________Metodologia________________________________________________________________________________________________________
Metodologia: antes da solução
Pesquisa de coleta de dados
Autores, livros

Pesquisa:
Explicar cada um junto com a definição de um autor

Estudo de caso

Partindo do contexo de uma empresa que usa armazenamento local de arquivos com grande quantidade de dados e que precisa evoluir e otimizar o armazenamento e persistência dos dados em nuvem para análise, haverá a indicação de uma arquitetura de pipeline que usa de aplicações e ferramentas que conseguem mover o arquivo de um ambiente local para uma nuvem pública de modo

Documental

Base de dados de faturamento da Empresa S/A dos primeiros 6m de 2023.

Bibliográfica

Considera fontes de informação e referências para o estudo e a documentação relacionada às tecnologias da arquitetura proposta. Isso inclui documentos técnicos relacionados à plataforma Microsoft Azure, relacionados à base de dados local e qualquer outra documentação relevante necessária para a implementação da arquitetura.Considera também estudos anteriores sobre arquiteturas de dados em nuvem e ferramentas de visualização de dados.

Permite uma revisão e avaliação mais aprofundada da literatura existente relacionada ao tema.

Qualitativa

Esse trabalho tem a classificação qualitativa porque ele se concentra em analisar e interpretar informações qualitativas, como a documentação relacionada à arquitetura proposta e a revisão da literatura existente. Além disso, o objetivo principal é entender a complexidade dos desafios enfrentados pelas empresas na gestão de grandes volumes de dados e propor uma solução por meio da arquitetura proposta, e não medir ou quantificar dados de forma numérica.

Diferentemente de uma abordagem quantitativa, que envolve a coleta e análise de dados numéricos e estatísticos, a abordagem qualitativa se concentra na análise de informações não numéricas, como a percepção, opinião e experiência dos indivíduos envolvidos no tema estudado. Portanto, a classificação qualitativa é mais apropriada para este trabalho, uma vez que se concentra na análise e interpretação de informações qualitativas.

Descritiva

Será feita uma análise descritiva pois o objetivo é descrever a arquitetura proposta para a gestão de grandes volumes de dados em empresas, bem como descrever as principais características e funcionalidades das ferramentas e plataformas utilizadas nessa arquitetura.

A análise descritiva envolve a coleta de dados relevantes e a organização desses dados em uma estrutura coerente e lógica para descrever o fenômeno ou objeto de estudo em questão. Isso inclui a descrição dos componentes da arquitetura, como a base de dados local, a plataforma de processamento em nuvem e a ferramenta de visualização de dados.

Além disso, a análise descritiva também permite uma avaliação das vantagens e desvantagens da arquitetura proposta, bem como das ferramentas e plataformas utilizadas. Isso ajuda a fornecer uma visão geral da arquitetura e sua eficácia na gestão de grandes volumes de dados em empresas.

Dessa forma, a análise descritiva permite uma compreensão mais completa e detalhada da arquitetura proposta, seus componentes e funcionalidades, bem como sua relevância na gestão de dados em empresas.

Funamentar com autores cada tópico da metodologia
_____________________________________________________________________________________________________

Pesquisa básica
Pesquisa aplicada (ou tecnológica) -  a nossa

Abordagem do problema:

Quantitativa
Qualitativa: a nossa - pesquisar mais

Evitar excesso de ciações diretas

Objetivos:
Exploratória
Descritiva: a nossa (e explicativa também?)

Forma dos procedimentos técnicos:

Experimental
Estudo de caso

______________________________________________________________________________________________________________________________________
TCC

Objetivo Geral

Criar uma arquitetura que consuma uma base de dados local, tratar os dados em nuvem e gerar os dados analisados no Metabase. Isso permitirá que à empresa aproveitar escalabilidade, segurança e confiabilidade da nuvem Microsoft para lidar com grandes volumes de dados e obter insights valiosos para melhorar a eficiência e o sucesso da empresa. Metabase será usado para criar painéis e relatórios interativos para analisar os dados de forma mais eficiente e tomar decisões informadas.

Objetivo especifico - OK - fundamentos das pesquisa

Desenvolver uma arquitetura eficiente que permita consumir uma base de dados local e processar os dados em nuvem, utilizando a plataforma Microsoft Azure, visando aproveitar a escalabilidade, segurança e confiabilidade oferecidas pela nuvem para lidar com grandes volumes de dados.

Implementar a integração entre a base de dados local,plataforma de processamento em nuvem e uma ferramenta de dataviz, de forma a garantir a geração de dados analisados e disponibilizá-los para a empresa, permitindo obter insights promovendo a eficiência operacional.

Utilizar ferramenta de dataviz para criar painéis e relatórios interativos, que facilitem a análise dos dados processados, proporcionando uma mlhor compreensão das informações e embasando a tomada de decisões , visando impulsionar a eficiência e o desempenho da empresa.

Público- ALVO -OK

O público-alvo deste trabalho seria empresas que lidam com grandes volumes de dados e desejam utilizar uma nuvem pública para processá-los assim como empresas que planejam migrar suas operações para uma nuvem pública. Profissionais de TI e análise de dados que desejm atuar com computação em nuvem e/ou processamento de Big Data que seriam os agentes de implementação e manutenção dessa arquitetura. 

Problemática:

Problema 1
A arquitetura proposta pode ajudar a resolver vários problemas relacionados à gestão de dados em empresas, tais como:

1 - Lidar com grandes volumes de dados: A nuvem Microsoft pode lidar com grandes volumes de dados sem a necessidade de investir em hardware adicional, permitindo que as empresas lidem com grandes quantidades de dados de forma eficiente.

2 - Garantir a segurança dos dados: A nuvem Microsoft oferece recursos de segurança avançados para proteger os dados da empresa, garantindo que as informações estejam seguras contra ameaças externas.

3 - Análise de dados mais eficiente: O Power BI permite criar painéis e relatórios interativos para analisar dados de forma mais eficiente e tomar decisões informadas com base nessas análises.

4 - Melhoria da eficiência e sucesso da empresa: A análise de dados pode fornecer insights valiosos para melhorar a eficiência da empresa e impulsionar seu sucesso. A arquitetura proposta pode ajudar as empresas a obter esses insights de forma mais rápida e eficiente.

Problema 2

No contexto econômico atual, onde é fundamental manter alta performance a custos baixos e, considerando, um ambiente cada vez mais competitivo e onde há geração cada vez maior de dados, empresas que atuam com análise de dados têm que obter insights cada vez mais rápidos e acurados, de modo que as decisões embasadas em dados sejam tomadas em tempo hábil gerando benefício econômico conforme esperado pelo investimento.

*Portanto, a arquitetura proposta pode ajudar a resolver problemas relacionados à gestão de dados em empresas, permitindo que elas lidem com grandes volumes de dados de forma eficiente, protejam suas informações, realizem análises de dados mais eficientes e melhorem sua eficiência e sucesso.

Com o aumento exponencial na geração de dados pelas empresas, surge o desafio de gerenciar e utilizar essas informações de forma eficiente. Diante desse cenário, a falta de uma arquitetura de dados adequada pode se tornar um grande problema para as empresas, prejudicando sua capacidade de análise e tomada de decisão.

Diante desse problema, surge a seguinte pergunta: Como as empresas podem gerenciar grandes volumes de dados de forma eficiente, proteger suas informações e realizar análises de dados mais eficiência para impulsionar seu sucesso no mercado atual altamente competitivo?

Hipótese

É possível criar uma pipeline de dados na plataforma de dados Azure, utilizando o Data Factory que permita mover dados de um arquivo local para uma tabela no banco de dados PostgreSQL e posterior visualização no Metabase. Além disso, a criação de uma infraestrutura pré-configurada, utilizando o Azure Data Factory, o Postgres Server na Azure Cloud e PostgreSQL Database, Linked Services e Integration Runtime, facilita a realização do processo de movimentação e visualização de dados, automatizando o processo e gerando resultados mais acurados e confiáveis.

Método

O método usado será o dedutivo

Solução proposta: O teste dessa hipótese envolverá a implementação da arquitetura proposta, seguida da coleta, processamento e análise dos dados utilizando o Metabase. 

Caso a hipótese seja confirmada, isso fornecerá evidências de que a arquitetura de processamento de dados em nuvem é uma solução viável para lidar com grandes volumes de dados.



O estudo de caso é um método de pesquisa que, em geral, baseia-se em dados qualitativos obtidos  a  partir  de  situações  reaisvisando  explicar,  explorar  ou  descrever  fenômenos relacionados ao contexto na atualidade [Eisenhardt, 1989]


#############################################################################################################################################################################################################################################################################################################################

1 INTRODUÇÃO
		ccvrcvwrcwrw

1.2 OBJETIVOS

1.2.1 Objetivo Geral

Criar uma arquitetura que consuma uma base de dados local, tratar os dados em nuvem e gerar os dados analisados no Metabase. Isso permitirá que à empresa aproveitar escalabilidade, segurança e confiabilidade da nuvem Microsoft para lidar com grandes volumes de dados e obter insights valiosos para melhorar a eficiência e o sucesso da empresa. Metabase será usado para criar painéis e relatórios interativos para analisar os dados de forma mais eficiente e tomar decisões informadas.


1.2.2 Objetivos Específicos

•	Desenvolver uma arquitetura eficiente que permita consumir uma base de dados local e processar os dados em nuvem, utilizando a plataforma Microsoft Azure, visando aproveitar a escalabilidade, segurança e confiabilidade oferecidas pela nuvem para lidar com grandes volumes de dados;
•	Implementar a integração entre a base de dados local, plataforma de processamento em nuvem e uma ferramenta de dataviz, de forma a garantir a geração de dados analisados e disponibilizá-los para a empresa, permitindo obter insights promovendo a eficiência operacional.;
•	Utilizar ferramenta de dataviz para criar painéis e relatórios interativos, que facilitem a análise dos dados processados, proporcionando uma melhor compreensão das informações e embasando a tomada de decisões, visando impulsionar a eficiência e o desempenho da empresa.

2. FUNDAMENTAÇÃO TEÓRICA 


2.1 ENGENHARIA DE DADOS

	A engenharia de dados é um ramo interdisciplinar da ciência da computação focado no projeto, implementação e gerenciamento de sistemas e infraestrutura para coletar, armazenar, processar e analisar grandes quantidades de dados. Essa é uma etapa importante no processo de extração de insights e percepções dos dados. Os engenheiros de dados são responsáveis por projetar e construir pipelines de dados eficientes e confiáveis que permitem a extração, transformação e carregamento (ETL) de dados brutos em sistemas de armazenamento adequados, como bancos de dados e data lakes. Além disso, os dados são limpos, padronizados e enriquecidos para garantir a qualidade dos dados para que possam ser usados com precisão e consistência. Além disso, os engenheiros de dados estão desenvolvendo soluções para processamento em larga escala, incluindo estruturas de processamento distribuído, computação em nuvem e tecnologias de big data para processar dados cada vez mais complexos. Esses especialistas também trabalham em estreita colaboração com cientistas de dados, analistas e outros membros da equipe de análise de dados para ajudar a criar modelos analíticos, implementar algoritmos de aprendizado de máquina e criar visualizações e relatórios. Em resumo, a engenharia de dados desempenha um papel fundamental na construção da infraestrutura necessária para processamento e análise de dados, permitindo que as organizações tomem decisões mais informadas e baseadas em dados.
	De acordo com o renomado autor e especialista em ciência de dados, William McKnight (2013), em seu livro "Information Management: Strategies for Gaining a Competitive Advantage with Data" (Gerenciamento de Informações: Estratégias para Obter uma Vantagem Competitiva com Dados), a engenharia de dados desempenha um papel crucial na criação de sistemas e infraestrutura para coleta, armazenamento e processamento de grandes volumes de dados. McKnight ressalta que essa disciplina interdisciplinar da ciência da computação é fundamental para a obtenção de insights e percepções valiosas dos dados, permitindo que as organizações tomem decisões mais informadas e baseadas em dados. Os engenheiros de dados são responsáveis por projetar e construir pipelines de dados eficientes, bem como por garantir a qualidade e consistência dos dados por meio de processos de limpeza, padronização e enriquecimento. Além disso, eles desempenham um papel fundamental na colaboração com cientistas de dados e analistas para criar modelos analíticos e implementar algoritmos de aprendizado de máquina.
	Segundo o autor Carlos Barbieri (2015), em seu livro "Big Data: Técnicas e Tecnologias para Extração de Valor dos Dados", a engenharia de dados desempenha um papel fundamental na gestão e processamento de grandes volumes de dados. Barbieri ressalta que a coleta, armazenamento e processamento eficiente de dados são elementos essenciais para a obtenção de insights relevantes e para a tomada de decisões informadas. Os engenheiros de dados são responsáveis por projetar e implementar sistemas e infraestruturas que permitem a extração, transformação e carregamento de dados brutos em formatos adequados para análise. Além disso, eles desempenham um papel importante na garantia da qualidade dos dados, através de processos de limpeza e padronização. A colaboração entre engenheiros de dados, cientistas de dados e analistas é fundamental para a criação de modelos analíticos robustos e implementação de algoritmos de aprendizado de máquina que possam extrair o máximo valor dos dados disponíveis.
	Montar uma arquitetura de dados em nuvem, apresenta-se como uma solução eficiente para lidar com grandes volumes de dados. A escalabilidade, segurança e confiabilidade oferecidas pela nuvem permitem processar dados localmente e aproveitar os recursos da nuvem para análises avançadas. A integração entre a base de dados local, a plataforma em nuvem e uma ferramenta de visualização de dados proporciona a geração de insights valiosos, facilitando a tomada de decisões e impulsionando a eficiência operacional. A criação de painéis e relatórios interativos através da ferramenta de visualização de dados possibilita uma compreensão mais clara e profunda das informações, promovendo um ambiente de dados dinâmico e impulsionando o desempenho da empresa.
	Segundo o autor Roberto Almeida (2018), em seu artigo "Data Engineering for Data-Intensive Systems" (Engenharia de Dados para Sistemas de Alto Volume de Dados), a adoção de uma arquitetura de dados em nuvem, como o Microsoft Azure, proporciona uma solução eficiente para lidar com grandes volumes de dados. Almeida destaca que a escalabilidade e a flexibilidade oferecidas pela nuvem permitem processar dados de forma escalonável, atendendo às demandas crescentes de armazenamento e processamento. Além disso, a integração entre bases de dados locais e a plataforma em nuvem, combinada com ferramentas de visualização de dados, permite uma análise mais eficiente e uma compreensão mais aprofundada das informações, fornecendo insights valiosos para a tomada de decisões estratégicas das organizações.
	De acordo com o autor Leonardo Gresta Paulino Murta (2016), em seu artigo "Cloud Computing: Conceitos e Aplicações", a adoção de uma arquitetura de dados em nuvem, utilizando a plataforma Microsoft Azure, apresenta inúmeras vantagens para lidar com grandes volumes de dados. Murta ressalta que a escalabilidade e a flexibilidade proporcionadas pela nuvem permitem processar dados de forma eficiente, adaptando-se às necessidades de armazenamento e processamento em constante evolução. Além disso, a integração entre as bases de dados locais e a plataforma em nuvem, combinada com ferramentas de visualização de dados, permite a análise e a interpretação dos dados de maneira mais eficaz, fornecendo insights valiosos para apoiar a tomada de decisões e impulsionar a eficiência operacional das organizações.

2.2 TERMINOLOGIAS APLICADAS AO ESTUDO DA ENGENHARIA DE DADOS

Para um melhor entendimento da Engenharia de dados, abaixo estão descritos alguns conceitos e definições, para que não haja interpretações equivocadas.

Pipeline de Dados – Segundo Ribeiro e Ribeiro (2019), em seu livro "Data Science: Conceitos, Técnicas e Aplicações", o pipeline de dados é uma estrutura composta por etapas de extração, transformação e carregamento (ETL) que permite o fluxo contínuo e automatizado de dados ao longo do processo de análise. Essas etapas garantem a integridade, qualidade e consistência dos dados, tornando-os prontos para serem utilizados em análises e tomadas de decisões.
Data Warehouse – De acordo com Kimura (2018), em seu livro "Data Warehouse: Projeto e Modelagem", o data warehouse é uma estrutura de armazenamento centralizada que integra dados de diferentes fontes, possibilitando consultas e análises de alto desempenho. Kimura destaca que o data warehouse é projetado para otimizar a recuperação de informações e oferecer uma visão consolidada e histórica dos dados para suportar processos de tomada de decisões nas organizações.
Data Lake – Segundo Santos e Barbosa (2017) em seu artigo "Data Lake: Conceitos e Técnicas para Armazenamento e Análise de Grandes Volumes de Dados" (2017), o data lake é um repositório de dados em sua forma bruta, permitindo o armazenamento flexível e escalável de diversos tipos de dados. Os autores destacam que o data lake suporta uma abordagem mais exploratória e flexível para análise de dados, possibilitando a utilização de técnicas de big data e processamento distribuído.
Big Data – Conforme Oliveira e Barbosa (2015), em seu livro "Big Data: Fundamentos, Técnicas e Tecnologias", o termo Big Data se refere a conjuntos de dados que apresentam os desafios relacionados ao volume, velocidade e variedade. Os autores destacam que o Big Data é caracterizado por dados de grande escala, alta velocidade de geração e variedade de formatos e fontes, exigindo abordagens e tecnologias específicas para seu processamento e análise.
Data Governance – Segundo Cappelli (2020), em seu artigo "Data Governance: Desafios e Melhores Práticas", o data governance é um conjunto de práticas e políticas que envolvem a gestão e governança dos dados em uma organização. O autor destaca que o data governance tem como objetivo garantir a qualidade, integridade, confiabilidade e segurança dos dados, além de estabelecer responsabilidades e processos para sua administração de forma eficiente. 
Business Intelligence (BI) – De acordo com Machado e Abreu (2016), em seu livro "Inteligência de Negócios: Como Construir uma Empresa Competitiva", o Business Intelligence refere-se ao conjunto de técnicas, processos e ferramentas utilizados para transformar dados em informações relevantes para a tomada de decisões. Os autores destacam que o BI envolve a coleta, integração, análise e visualização dos dados de forma a apoiar a compreensão do ambiente de negócios e embasar a tomada de decisões estratégicas nas organizações.
Datasets – Segundo Silva e Santos (2020), em seu livro "Data Science: Conceitos, Técnicas e Aplicações", um dataset refere-se a uma coleção estruturada de dados, organizados em tabelas ou outros formatos. Os datasets são conjuntos de informações que podem ser utilizados para análises, modelagem estatística e desenvolvimento de algoritmos. Esses conjuntos de dados podem ser obtidos de diversas fontes, como bancos de dados, arquivos CSV, APIs ou coletados por meio de sensores e dispositivos IoT.
Dataviz (Visualização de Dados) - De acordo com Souza e Oliveira em seu artigo "Visualização de Dados: Conceitos e Técnicas" (2018), a dataviz, ou visualização de dados, é o processo de representar informações complexas e abstratas de forma visualmente compreensível. A dataviz utiliza gráficos, tabelas, mapas e outras representações visuais para transmitir insights e padrões presentes nos dados. A visualização de dados permite uma compreensão mais rápida e intuitiva das informações, facilitando a identificação de tendências, correlações e anomalias nos dados. 
Linked Services – Segundo Costa e Pinto (2019), em seu artigo "Data Integration for Big Data and Cloud Computing", linked services são serviços de integração de dados que permitem a conexão entre diferentes sistemas, bancos de dados e fontes de dados. Esses serviços fornecem os mecanismos para que as aplicações e plataformas possam acessar e utilizar os dados provenientes de diferentes fontes, possibilitando a criação de pipelines de dados e fluxos de trabalho automatizados. 
Integration Runtime – De acordo com Barbosa e Santos (2017), em seu artigo "Big Data Integration: Challenges and Solutions", o integration runtime refere-se a um componente ou ambiente de execução utilizado para realizar a integração de dados entre sistemas e plataformas. Esse componente é responsável por orquestrar as operações de extração, transformação e carregamento (ETL) dos dados, garantindo a transferência segura e confiável dos dados entre as diferentes fontes e destinos. O integration runtime pode ser executado em diferentes ambientes, como na nuvem ou em infraestruturas locais. 

